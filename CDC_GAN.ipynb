{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngBaz/Generative-AI-GANs/blob/main/CDC_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2HKez_IERgn",
        "outputId": "71bdcfc1-078a-44e2-f941-4c325c0a7c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjNIgMaMiOxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d74f08b-1d31-4581-b377-ae16d88b4c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-040eaac0f986>:96: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  fixed_z_, fixed_y_label_ = Variable(fixed_z_.to(device), volatile=True), Variable(fixed_y_label_.to(device), volatile=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 199864970.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 23977769.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 142872606.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22924824.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "training start!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [04:08<00:00,  3.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/5] - ptime: 248.85, loss_d: 0.734, loss_g: 2.915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [06:59<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/5] - ptime: 419.05, loss_d: 0.972, loss_g: 1.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [09:58<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3/5] - ptime: 598.31, loss_d: 1.018, loss_g: 1.683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [13:00<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4/5] - ptime: 780.70, loss_d: 0.860, loss_g: 1.944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [16:16<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5/5] - ptime: 976.87, loss_d: 0.635, loss_g: 2.451\n",
            "Avg one epoch ptime: 604.76, total 5 epochs ptime: 3054.06\n",
            "Training finish!... save training results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-040eaac0f986>:314: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  images.append(imageio.imread(img_name))\n"
          ]
        }
      ],
      "source": [
        "import os, time\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# G(z)\n",
        "class generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d=128):\n",
        "        super(generator, self).__init__()\n",
        "        self.deconv1_1 = nn.ConvTranspose2d(100, d*2, 4, 1, 0)\n",
        "        self.deconv1_1_bn = nn.BatchNorm2d(d*2)\n",
        "        self.deconv1_2 = nn.ConvTranspose2d(10, d*2, 4, 1, 0)\n",
        "        self.deconv1_2_bn = nn.BatchNorm2d(d*2)\n",
        "        self.deconv2 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
        "        self.deconv2_bn = nn.BatchNorm2d(d*2)\n",
        "        self.deconv3 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
        "        self.deconv3_bn = nn.BatchNorm2d(d)\n",
        "        self.deconv4 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input, label):\n",
        "        x = F.relu(self.deconv1_1_bn(self.deconv1_1(input)))\n",
        "        y = F.relu(self.deconv1_2_bn(self.deconv1_2(label)))\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
        "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
        "        x = torch.tanh(self.deconv4(x))\n",
        "        # x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
        "        # x = F.tanh(self.deconv5(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d=128):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(1, d//2, 4, 2, 1)\n",
        "        self.conv1_2 = nn.Conv2d(10, d//2, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
        "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
        "        self.conv4 = nn.Conv2d(d * 4, 1, 4, 1, 0)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input, label):\n",
        "        x = F.leaky_relu(self.conv1_1(input), 0.2)\n",
        "        y = F.leaky_relu(self.conv1_2(label), 0.2)\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
        "        x = torch.sigmoid(self.conv4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "# fixed noise & label\n",
        "temp_z_ = torch.randn(10, 100)\n",
        "fixed_z_ = temp_z_\n",
        "fixed_y_ = torch.zeros(10, 1)\n",
        "for i in range(9):\n",
        "    fixed_z_ = torch.cat([fixed_z_, temp_z_], 0)\n",
        "    temp = torch.ones(10, 1) + i\n",
        "    fixed_y_ = torch.cat([fixed_y_, temp], 0)\n",
        "\n",
        "fixed_z_ = fixed_z_.view(-1, 100, 1, 1)\n",
        "fixed_y_label_ = torch.zeros(100, 10)\n",
        "fixed_y_label_.scatter_(1, fixed_y_.type(torch.LongTensor), 1)\n",
        "fixed_y_label_ = fixed_y_label_.view(-1, 10, 1, 1)\n",
        "fixed_z_, fixed_y_label_ = Variable(fixed_z_.to(device), volatile=True), Variable(fixed_y_label_.to(device), volatile=True)\n",
        "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
        "\n",
        "    G.eval()\n",
        "    test_images = G(fixed_z_, fixed_y_label_)\n",
        "    G.train()\n",
        "\n",
        "    size_figure_grid = 10\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(10*10):\n",
        "        i = k // 10\n",
        "        j = k % 10\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "    plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "# training parameters\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "train_epoch = 5\n",
        "\n",
        "# data_loader\n",
        "img_size = 32\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# network\n",
        "G = generator(128)#torch.jit.load('/content/gdrive/MyDrive/Assignment_2_Generative_models/CDCGAN/saved_folders/G_scripted_11.pt',map_location=device)#generator(128)\n",
        "D = discriminator(128)#torch.jit.load('/content/gdrive/MyDrive/Assignment_2_Generative_models/CDCGAN/saved_folders/D_scripted_11.pt',map_location=device)#discriminator(128)\n",
        "G.weight_init(mean=0.0, std=0.02)\n",
        "D.weight_init(mean=0.0, std=0.02)\n",
        "G.to(device)\n",
        "D.to(device)\n",
        "\n",
        "# Binary Cross Entropy loss\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "# Adam optimizer\n",
        "G_optimizer = optim.AdamW(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.AdamW(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# results save folder\n",
        "root = '/content/gdrive/MyDrive/CDC_GAN/'\n",
        "model = 'MNIST_cDCGAN_'\n",
        "save_dir = '/content/gdrive/MyDrive/CDC_GAN/'\n",
        "\n",
        "if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "if not os.path.isdir(root + 'Fixed_results'):\n",
        "    os.mkdir(root + 'Fixed_results')\n",
        "\n",
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []\n",
        "train_hist['per_epoch_ptimes'] = []\n",
        "train_hist['total_ptime'] = []\n",
        "\n",
        "# label preprocess\n",
        "onehot = torch.zeros(10, 10)\n",
        "onehot = onehot.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1).view(10, 10, 1, 1)\n",
        "fill = torch.zeros([10, 10, img_size, img_size])\n",
        "for i in range(10):\n",
        "    fill[i, i, :, :] = 1\n",
        "\n",
        "print('training start!')\n",
        "start_time = time.time()\n",
        "for epoch in range(train_epoch):\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    # learning rate decay\n",
        "    if (epoch+1) == 11:\n",
        "        G_optimizer.param_groups[0]['lr'] /= 10\n",
        "        D_optimizer.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "    if (epoch+1) == 16:\n",
        "        G_optimizer.param_groups[0]['lr'] /= 10\n",
        "        D_optimizer.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    y_real_ = torch.ones(batch_size)\n",
        "    y_fake_ = torch.zeros(batch_size)\n",
        "    y_real_, y_fake_ = Variable(y_real_.to(device)), Variable(y_fake_.to(device))\n",
        "    for x_, y_ in tqdm(train_loader):\n",
        "        gc.collect()\n",
        "        # train discriminator D\n",
        "        D.zero_grad()\n",
        "\n",
        "        mini_batch = x_.size()[0]\n",
        "\n",
        "        if mini_batch != batch_size:\n",
        "            y_real_ = torch.ones(mini_batch)\n",
        "            y_fake_ = torch.zeros(mini_batch)\n",
        "            y_real_, y_fake_ = Variable(y_real_.to(device)), Variable(y_fake_.to(device))\n",
        "\n",
        "        y_fill_ = fill[y_]\n",
        "        x_, y_fill_ = Variable(x_.to(device)), Variable(y_fill_.to(device))\n",
        "\n",
        "        D_result = D(x_, y_fill_).squeeze()\n",
        "        D_real_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n",
        "        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor).squeeze()\n",
        "        y_label_ = onehot[y_]\n",
        "        y_fill_ = fill[y_]\n",
        "        z_, y_label_, y_fill_ = Variable(z_.to(device)), Variable(y_label_.to(device)), Variable(y_fill_.to(device))\n",
        "\n",
        "        G_result = G(z_, y_label_)\n",
        "        D_result = D(G_result, y_fill_).squeeze()\n",
        "\n",
        "        D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "        D_fake_score = D_result.data.mean()\n",
        "\n",
        "        D_train_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_train_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        D_losses.append(D_train_loss.data)\n",
        "\n",
        "        # train generator G\n",
        "        G.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n",
        "        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor).squeeze()\n",
        "        y_label_ = onehot[y_]\n",
        "        y_fill_ = fill[y_]\n",
        "        z_, y_label_, y_fill_ = Variable(z_.cuda()), Variable(y_label_.cuda()), Variable(y_fill_.cuda())\n",
        "\n",
        "        G_result = G(z_, y_label_)\n",
        "        D_result = D(G_result, y_fill_).squeeze()\n",
        "\n",
        "        G_train_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        G_train_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_train_loss.data)\n",
        "        gc.collect()\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "    print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses))))\n",
        "    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
        "    show_result((epoch+1), save=True, path=fixed_p)\n",
        "    train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
        "    train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "    if epoch%5 == 0:\n",
        "      G_scripted = torch.jit.script(G) # Export to TorchScript\n",
        "      G_scripted.save(os.path.join(save_dir, 'G_scripted_{}.pt'.format(epoch)))\n",
        "      D_scripted = torch.jit.script(D) # Export to TorchScript\n",
        "      D_scripted.save(os.path.join(save_dir, 'D_scripted_{}.pt'.format(epoch)))\n",
        "\n",
        "end_time = time.time()\n",
        "total_ptime = end_time - start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "print(\"Training finish!... save training results\")\n",
        "torch.save(G.state_dict(), root + model + 'generator_param.pkl')\n",
        "torch.save(D.state_dict(), root + model + 'discriminator_param.pkl')\n",
        "with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
        "\n",
        "images = []\n",
        "for e in range(train_epoch):\n",
        "    img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "    images.append(imageio.imread(img_name))\n",
        "imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)"
      ]
    }
  ]
}